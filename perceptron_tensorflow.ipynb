{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "perceptron tensorflow.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacr2006/tensorflow/blob/master/perceptron_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNVMJWROy7Ks",
        "colab_type": "text"
      },
      "source": [
        "##Perceptron\n",
        "\n",
        "El propósito de este notebook es entrenar un Perceptrón (Red Neuronal de una capa) para un problema de clasificación multiclase de digitos escritos a mano. Finalmente se tratará de inferir la clase a la que pertenece una imagen de un digito escritos a mano. Para el entrenamiento usaremos el dataset MNIST conformado por 60.000 imagenes de entrenamiento y 10.000 imagenes de prueba.\n",
        "\n",
        "Es posible usar un Perceptron para este problema porque de antemano sabemos que el conjunto de entranamiento el linealmente separable.\n",
        "Este dataset esta disponible en la libreria \"tensorflow_datasets\" de tensorflow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zpbK3ahzsEX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q tensorflow tensorflow-datasets matplotlib"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Webb6mWzXrmZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmpqvCuTe7ZB",
        "colab_type": "text"
      },
      "source": [
        "Importar las librerias necesarias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zY_tbRdcgwa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmWD9Bh7lMc7",
        "colab_type": "text"
      },
      "source": [
        "###Datos\n",
        "En este dataset la data de entrada es un conjunto imagenes estan centradas y estandarizadas a un mismo tamaño (28X28x1 pixels, en blanco y negro), no es requerida limpieza de los datos. La data de salida esta conformada por un conjuto finito de datos etiquetados entre el valor 0 y el 9, data categórica.\n",
        "Lectura de los datos de entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcHVhWLQ1xM2",
        "colab_type": "code",
        "outputId": "49f473d6-9035-41d9-c0ee-72d84317d8da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "train_ds = tfds.load(\"mnist\", split=tfds.Split.TRAIN, batch_size=-1)\n",
        "numpy_ds = tfds.as_numpy(train_ds)\n",
        "numpy_images, numpy_labels = numpy_ds[\"image\"], numpy_ds[\"label\"]"
      ],
      "execution_count": 337,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0901 00:43:50.292904 140523261151104 dataset_builder.py:439] Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Kxoiod2_hjK",
        "colab_type": "code",
        "outputId": "8ed476b7-2093-4128-9692-188217262824",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "numpy_images.shape"
      ],
      "execution_count": 338,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 338
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAE803OqmPx2",
        "colab_type": "text"
      },
      "source": [
        "Como la entrada al Perceptron debe ser de una dimension, se requiere una transformacion de la data de entrada de 60000x28x28x1 a 60000x784"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRzVt5hYbByQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "numpy_images=numpy_images.reshape(60000,784)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSHsUaUpUqgs",
        "colab_type": "code",
        "outputId": "77618ecd-d89a-4c6a-8df6-6b33482ad3b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "numpy_images.shape"
      ],
      "execution_count": 340,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 340
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgclhEqP9yNt",
        "colab_type": "text"
      },
      "source": [
        "La siguiente figura muestra la imagen de un numero perteneciente al conjunto de entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzbUa6Rrtebk",
        "colab_type": "code",
        "outputId": "0516fd97-ef72-4ec0-c64b-94253d644232",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "image_index = 42 \n",
        "img=numpy_images[image_index].reshape(28,28)\n",
        "plt.imshow(img, cmap=\"gray\")"
      ],
      "execution_count": 341,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fcd50788470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 341
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_KN0Yw6DGLD",
        "colab_type": "code",
        "outputId": "bd48ad4c-3add-4ab5-8660-07caeb5b24ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "numpy_labels.shape"
      ],
      "execution_count": 342,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 342
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxpqZOi4nIe3",
        "colab_type": "text"
      },
      "source": [
        "Lectura de los datos de prueba"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJhRTr97Z7uV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_ds = tfds.load(\"mnist\", split=tfds.Split.TEST, batch_size=-1)\n",
        "numpy_ds_test = tfds.as_numpy(test_ds)\n",
        "numpy_images_test, numpy_labels_test = numpy_ds_test[\"image\"], numpy_ds_test[\"label\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzWroSUra6lp",
        "colab_type": "code",
        "outputId": "f0870521-505d-4bc4-926d-9d7c4d2a393f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "numpy_images_test.shape"
      ],
      "execution_count": 344,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 344
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTvF0A92azy5",
        "colab_type": "code",
        "outputId": "8f86ca21-7bc2-437d-ed0a-365e52a0d7aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "numpy_images_test=numpy_images_test.reshape(10000,784)\n",
        "numpy_images_test.shape"
      ],
      "execution_count": 345,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 345
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pV7pwioKNW_r",
        "outputId": "4c5b01e9-209d-473a-f031-10edd8cb0c22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "sess = tf.InteractiveSession()"
      ],
      "execution_count": 346,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1735: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvTrEfCZzO7G",
        "colab_type": "text"
      },
      "source": [
        "Los algoritmos de \"machine learning\" no funcionan bien con data categorica directamente. Por tanto, se deben codificar estas categorias numéricas en vectores binarios. Para este caso donde existen 10 clases de salida representadas por un valor decimal entre 0 y 9, se codifican en un vector binario 1x10 donde solo existe una columna con un valor 1 mientras el resto son 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Th2gOhq2ETUA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "numpy_labels=tf.one_hot(numpy_labels, depth=10)\n",
        "labels=sess.run(numpy_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Qp1qcaH41az",
        "colab_type": "code",
        "outputId": "c4205f72-bd51-41f5-eedb-5f884070b419",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "labels.shape"
      ],
      "execution_count": 348,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 348
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UsfdPiq2_6V",
        "colab_type": "text"
      },
      "source": [
        "Se realiza la misma codificación para el conjunto de prueba"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gX1U2nuaRZu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "numpy_labels_test=tf.one_hot(numpy_labels_test, depth=10)\n",
        "labels_test=sess.run(numpy_labels_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSVD5fl9M3Vd",
        "colab_type": "code",
        "outputId": "1d642c1f-8162-4996-a2d9-5eece05c623c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "labels_test.shape"
      ],
      "execution_count": 350,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 350
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2IDtYKO5JBc",
        "colab_type": "text"
      },
      "source": [
        "###Entrenamiento\n",
        "Se crean \"placeHolders\" para la variables de entrada (x) y la de salida (y_)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVg2nfu-ETgX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x  = tf.placeholder(tf.float32, shape=[None, 784])\n",
        "y_ = tf.placeholder(tf.float32, shape=[None, 10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxfYy0aeCT6J",
        "colab_type": "text"
      },
      "source": [
        "Se crean las variables del modelo del perceptron y = x * W + b"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQXXiVl2FTiq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "W = tf.Variable(tf.zeros([784, 10],tf.float32))\n",
        "b = tf.Variable(tf.zeros([10],tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVdQK5j4eHdw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WE1_Nkw3nze",
        "colab_type": "text"
      },
      "source": [
        "Por definicion el Perceptron utiliza como función de activación \"softmax\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keLF0TVEEf2_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = tf.nn.softmax(tf.add(tf.matmul(x,W), b))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d68MC6swEkzM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7D5k00Z16wR6",
        "colab_type": "text"
      },
      "source": [
        "Para este caso, se usa Gradiente Descendente como algoritmo de optimización"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFXZe5UJEk2-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate=0.00001#se selecciona un \"learning rate\" pequeño, y se observa la convergencia\n",
        "optimizer=tf.train.GradientDescentOptimizer(learning_rate)\n",
        "train_step = optimizer.minimize(cross_entropy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s95UpkeJ7SnS",
        "colab_type": "text"
      },
      "source": [
        "Se ejecutan un conjunto finito de iteraciones, y se analiza la convergencia mediendo la diferencia entre dos sucesivos valores de costo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtcwWfIdEnyZ",
        "colab_type": "code",
        "outputId": "d8ae61c7-2658-4bfe-97ec-4910bb4ec478",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "cost=0\n",
        "for i in range(1000):\n",
        "    step, new_cross_entropy, new_W, new_b = sess.run([train_step, cross_entropy, W, b], feed_dict={x: numpy_images, y_: labels})\n",
        "    diff=abs(cost-new_cross_entropy)\n",
        "    cost=new_cross_entropy\n",
        "    print(\"Paso: {}, Costo: {}\".format(i, new_cross_entropy) )\n",
        "    if i > 1 and diff < .0001:#usamos un criterio de convergencia de 10-3 para finalizar el entrenamiento\n",
        "      print(\"El entrenamiento converge\")\n",
        "      break\n",
        "    "
      ],
      "execution_count": 357,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Paso: 0, Costo: 2.3025853633880615\n",
            "Paso: 1, Costo: 1.719786286354065\n",
            "Paso: 2, Costo: 1.3874725103378296\n",
            "Paso: 3, Costo: 1.1929572820663452\n",
            "Paso: 4, Costo: 1.0867972373962402\n",
            "Paso: 5, Costo: 0.9944651126861572\n",
            "Paso: 6, Costo: 0.9694867134094238\n",
            "Paso: 7, Costo: 0.8473393321037292\n",
            "Paso: 8, Costo: 0.8233635425567627\n",
            "Paso: 9, Costo: 0.7672825455665588\n",
            "Paso: 10, Costo: 0.7526967525482178\n",
            "Paso: 11, Costo: 0.70330411195755\n",
            "Paso: 12, Costo: 0.6863643527030945\n",
            "Paso: 13, Costo: 0.6537224054336548\n",
            "Paso: 14, Costo: 0.6370964646339417\n",
            "Paso: 15, Costo: 0.6150473952293396\n",
            "Paso: 16, Costo: 0.6004779934883118\n",
            "Paso: 17, Costo: 0.5851417183876038\n",
            "Paso: 18, Costo: 0.5731908679008484\n",
            "Paso: 19, Costo: 0.5619328618049622\n",
            "Paso: 20, Costo: 0.5523132681846619\n",
            "Paso: 21, Costo: 0.5435487627983093\n",
            "Paso: 22, Costo: 0.5357035994529724\n",
            "Paso: 23, Costo: 0.5285320281982422\n",
            "Paso: 24, Costo: 0.5219516158103943\n",
            "Paso: 25, Costo: 0.515857458114624\n",
            "Paso: 26, Costo: 0.5101805925369263\n",
            "Paso: 27, Costo: 0.5048637390136719\n",
            "Paso: 28, Costo: 0.4998635947704315\n",
            "Paso: 29, Costo: 0.4951454997062683\n",
            "Paso: 30, Costo: 0.49068164825439453\n",
            "Paso: 31, Costo: 0.4864487051963806\n",
            "Paso: 32, Costo: 0.48242706060409546\n",
            "Paso: 33, Costo: 0.47859957814216614\n",
            "Paso: 34, Costo: 0.4749511778354645\n",
            "Paso: 35, Costo: 0.47146832942962646\n",
            "Paso: 36, Costo: 0.46813908219337463\n",
            "Paso: 37, Costo: 0.4649525284767151\n",
            "Paso: 38, Costo: 0.46189892292022705\n",
            "Paso: 39, Costo: 0.4589692950248718\n",
            "Paso: 40, Costo: 0.45615559816360474\n",
            "Paso: 41, Costo: 0.45345044136047363\n",
            "Paso: 42, Costo: 0.4508470594882965\n",
            "Paso: 43, Costo: 0.4483392834663391\n",
            "Paso: 44, Costo: 0.44592148065567017\n",
            "Paso: 45, Costo: 0.44358834624290466\n",
            "Paso: 46, Costo: 0.44133514165878296\n",
            "Paso: 47, Costo: 0.43915748596191406\n",
            "Paso: 48, Costo: 0.4370511770248413\n",
            "Paso: 49, Costo: 0.43501242995262146\n",
            "Paso: 50, Costo: 0.43303775787353516\n",
            "Paso: 51, Costo: 0.43112388253211975\n",
            "Paso: 52, Costo: 0.4292677640914917\n",
            "Paso: 53, Costo: 0.42746657133102417\n",
            "Paso: 54, Costo: 0.42571762204170227\n",
            "Paso: 55, Costo: 0.42401841282844543\n",
            "Paso: 56, Costo: 0.42236676812171936\n",
            "Paso: 57, Costo: 0.4207603931427002\n",
            "Paso: 58, Costo: 0.41919729113578796\n",
            "Paso: 59, Costo: 0.41767555475234985\n",
            "Paso: 60, Costo: 0.416193425655365\n",
            "Paso: 61, Costo: 0.4147491157054901\n",
            "Paso: 62, Costo: 0.41334113478660583\n",
            "Paso: 63, Costo: 0.41196802258491516\n",
            "Paso: 64, Costo: 0.4106282591819763\n",
            "Paso: 65, Costo: 0.40932056307792664\n",
            "Paso: 66, Costo: 0.40804368257522583\n",
            "Paso: 67, Costo: 0.4067963659763336\n",
            "Paso: 68, Costo: 0.40557757019996643\n",
            "Paso: 69, Costo: 0.4043861925601959\n",
            "Paso: 70, Costo: 0.4032211899757385\n",
            "Paso: 71, Costo: 0.4020816385746002\n",
            "Paso: 72, Costo: 0.400966614484787\n",
            "Paso: 73, Costo: 0.3998751938343048\n",
            "Paso: 74, Costo: 0.3988065719604492\n",
            "Paso: 75, Costo: 0.3977600336074829\n",
            "Paso: 76, Costo: 0.3967347741127014\n",
            "Paso: 77, Costo: 0.39573007822036743\n",
            "Paso: 78, Costo: 0.39474526047706604\n",
            "Paso: 79, Costo: 0.3937796950340271\n",
            "Paso: 80, Costo: 0.3928326964378357\n",
            "Paso: 81, Costo: 0.3919037878513336\n",
            "Paso: 82, Costo: 0.39099225401878357\n",
            "Paso: 83, Costo: 0.3900977075099945\n",
            "Paso: 84, Costo: 0.3892195224761963\n",
            "Paso: 85, Costo: 0.3883572220802307\n",
            "Paso: 86, Costo: 0.38751035928726196\n",
            "Paso: 87, Costo: 0.38667842745780945\n",
            "Paso: 88, Costo: 0.38586103916168213\n",
            "Paso: 89, Costo: 0.3850577473640442\n",
            "Paso: 90, Costo: 0.3842681646347046\n",
            "Paso: 91, Costo: 0.3834918737411499\n",
            "Paso: 92, Costo: 0.3827285170555115\n",
            "Paso: 93, Costo: 0.38197776675224304\n",
            "Paso: 94, Costo: 0.38123923540115356\n",
            "Paso: 95, Costo: 0.3805125951766968\n",
            "Paso: 96, Costo: 0.3797975778579712\n",
            "Paso: 97, Costo: 0.37909382581710815\n",
            "Paso: 98, Costo: 0.3784010410308838\n",
            "Paso: 99, Costo: 0.3777189552783966\n",
            "Paso: 100, Costo: 0.3770473301410675\n",
            "Paso: 101, Costo: 0.37638580799102783\n",
            "Paso: 102, Costo: 0.3757341802120209\n",
            "Paso: 103, Costo: 0.3750922381877899\n",
            "Paso: 104, Costo: 0.37445974349975586\n",
            "Paso: 105, Costo: 0.3738363981246948\n",
            "Paso: 106, Costo: 0.3732220232486725\n",
            "Paso: 107, Costo: 0.37261641025543213\n",
            "Paso: 108, Costo: 0.37201935052871704\n",
            "Paso: 109, Costo: 0.3714306056499481\n",
            "Paso: 110, Costo: 0.3708500266075134\n",
            "Paso: 111, Costo: 0.37027737498283386\n",
            "Paso: 112, Costo: 0.36971256136894226\n",
            "Paso: 113, Costo: 0.36915531754493713\n",
            "Paso: 114, Costo: 0.36860549449920654\n",
            "Paso: 115, Costo: 0.36806294322013855\n",
            "Paso: 116, Costo: 0.36752748489379883\n",
            "Paso: 117, Costo: 0.3669990003108978\n",
            "Paso: 118, Costo: 0.36647728085517883\n",
            "Paso: 119, Costo: 0.36596226692199707\n",
            "Paso: 120, Costo: 0.3654537498950958\n",
            "Paso: 121, Costo: 0.36495158076286316\n",
            "Paso: 122, Costo: 0.3644556999206543\n",
            "Paso: 123, Costo: 0.3639658987522125\n",
            "Paso: 124, Costo: 0.3634820580482483\n",
            "Paso: 125, Costo: 0.3630041480064392\n",
            "Paso: 126, Costo: 0.36253196001052856\n",
            "Paso: 127, Costo: 0.3620653748512268\n",
            "Paso: 128, Costo: 0.3616043031215668\n",
            "Paso: 129, Costo: 0.3611486554145813\n",
            "Paso: 130, Costo: 0.36069831252098083\n",
            "Paso: 131, Costo: 0.3602531850337982\n",
            "Paso: 132, Costo: 0.3598131537437439\n",
            "Paso: 133, Costo: 0.3593780994415283\n",
            "Paso: 134, Costo: 0.3589479923248291\n",
            "Paso: 135, Costo: 0.3585226535797119\n",
            "Paso: 136, Costo: 0.358102023601532\n",
            "Paso: 137, Costo: 0.3576861023902893\n",
            "Paso: 138, Costo: 0.3572746813297272\n",
            "Paso: 139, Costo: 0.3568677604198456\n",
            "Paso: 140, Costo: 0.3564651608467102\n",
            "Paso: 141, Costo: 0.35606691241264343\n",
            "Paso: 142, Costo: 0.3556729257106781\n",
            "Paso: 143, Costo: 0.35528308153152466\n",
            "Paso: 144, Costo: 0.35489726066589355\n",
            "Paso: 145, Costo: 0.3545154929161072\n",
            "Paso: 146, Costo: 0.35413768887519836\n",
            "Paso: 147, Costo: 0.35376372933387756\n",
            "Paso: 148, Costo: 0.3533935546875\n",
            "Paso: 149, Costo: 0.35302719473838806\n",
            "Paso: 150, Costo: 0.35266444087028503\n",
            "Paso: 151, Costo: 0.3523053526878357\n",
            "Paso: 152, Costo: 0.3519497513771057\n",
            "Paso: 153, Costo: 0.35159772634506226\n",
            "Paso: 154, Costo: 0.351249098777771\n",
            "Paso: 155, Costo: 0.35090383887290955\n",
            "Paso: 156, Costo: 0.3505619168281555\n",
            "Paso: 157, Costo: 0.35022327303886414\n",
            "Paso: 158, Costo: 0.349887877702713\n",
            "Paso: 159, Costo: 0.349555641412735\n",
            "Paso: 160, Costo: 0.3492265045642853\n",
            "Paso: 161, Costo: 0.3489004671573639\n",
            "Paso: 162, Costo: 0.3485774099826813\n",
            "Paso: 163, Costo: 0.3482573628425598\n",
            "Paso: 164, Costo: 0.34794023633003235\n",
            "Paso: 165, Costo: 0.3476259708404541\n",
            "Paso: 166, Costo: 0.34731459617614746\n",
            "Paso: 167, Costo: 0.3470059931278229\n",
            "Paso: 168, Costo: 0.34670013189315796\n",
            "Paso: 169, Costo: 0.3463970124721527\n",
            "Paso: 170, Costo: 0.34609654545783997\n",
            "Paso: 171, Costo: 0.3457987606525421\n",
            "Paso: 172, Costo: 0.3455035090446472\n",
            "Paso: 173, Costo: 0.34521085023880005\n",
            "Paso: 174, Costo: 0.34492069482803345\n",
            "Paso: 175, Costo: 0.3446330726146698\n",
            "Paso: 176, Costo: 0.34434786438941956\n",
            "Paso: 177, Costo: 0.3440650403499603\n",
            "Paso: 178, Costo: 0.3437846302986145\n",
            "Paso: 179, Costo: 0.3435065746307373\n",
            "Paso: 180, Costo: 0.3432307839393616\n",
            "Paso: 181, Costo: 0.34295734763145447\n",
            "Paso: 182, Costo: 0.34268608689308167\n",
            "Paso: 183, Costo: 0.3424171209335327\n",
            "Paso: 184, Costo: 0.3421502709388733\n",
            "Paso: 185, Costo: 0.34188562631607056\n",
            "Paso: 186, Costo: 0.34162306785583496\n",
            "Paso: 187, Costo: 0.3413626253604889\n",
            "Paso: 188, Costo: 0.34110429883003235\n",
            "Paso: 189, Costo: 0.3408479690551758\n",
            "Paso: 190, Costo: 0.34059369564056396\n",
            "Paso: 191, Costo: 0.34034135937690735\n",
            "Paso: 192, Costo: 0.3400910198688507\n",
            "Paso: 193, Costo: 0.33984264731407166\n",
            "Paso: 194, Costo: 0.339596152305603\n",
            "Paso: 195, Costo: 0.3393515348434448\n",
            "Paso: 196, Costo: 0.33910879492759705\n",
            "Paso: 197, Costo: 0.3388679027557373\n",
            "Paso: 198, Costo: 0.3386288285255432\n",
            "Paso: 199, Costo: 0.33839151263237\n",
            "Paso: 200, Costo: 0.3381560146808624\n",
            "Paso: 201, Costo: 0.33792227506637573\n",
            "Paso: 202, Costo: 0.33769020438194275\n",
            "Paso: 203, Costo: 0.33745989203453064\n",
            "Paso: 204, Costo: 0.33723124861717224\n",
            "Paso: 205, Costo: 0.33700424432754517\n",
            "Paso: 206, Costo: 0.3367789089679718\n",
            "Paso: 207, Costo: 0.33655521273612976\n",
            "Paso: 208, Costo: 0.3363330662250519\n",
            "Paso: 209, Costo: 0.3361125588417053\n",
            "Paso: 210, Costo: 0.3358936309814453\n",
            "Paso: 211, Costo: 0.3356761932373047\n",
            "Paso: 212, Costo: 0.3354603052139282\n",
            "Paso: 213, Costo: 0.3352459669113159\n",
            "Paso: 214, Costo: 0.3350330591201782\n",
            "Paso: 215, Costo: 0.3348216712474823\n",
            "Paso: 216, Costo: 0.334611713886261\n",
            "Paso: 217, Costo: 0.33440324664115906\n",
            "Paso: 218, Costo: 0.33419615030288696\n",
            "Paso: 219, Costo: 0.3339904844760895\n",
            "Paso: 220, Costo: 0.3337862193584442\n",
            "Paso: 221, Costo: 0.3335833251476288\n",
            "Paso: 222, Costo: 0.3333818018436432\n",
            "Paso: 223, Costo: 0.33318158984184265\n",
            "Paso: 224, Costo: 0.33298274874687195\n",
            "Paso: 225, Costo: 0.3327851891517639\n",
            "Paso: 226, Costo: 0.33258897066116333\n",
            "Paso: 227, Costo: 0.332394003868103\n",
            "Paso: 228, Costo: 0.3322003185749054\n",
            "Paso: 229, Costo: 0.33200788497924805\n",
            "Paso: 230, Costo: 0.33181673288345337\n",
            "Paso: 231, Costo: 0.3316267430782318\n",
            "Paso: 232, Costo: 0.3314380347728729\n",
            "Paso: 233, Costo: 0.33125051856040955\n",
            "Paso: 234, Costo: 0.3310641944408417\n",
            "Paso: 235, Costo: 0.3308790326118469\n",
            "Paso: 236, Costo: 0.3306950628757477\n",
            "Paso: 237, Costo: 0.3305121660232544\n",
            "Paso: 238, Costo: 0.3303305208683014\n",
            "Paso: 239, Costo: 0.33014994859695435\n",
            "Paso: 240, Costo: 0.32997050881385803\n",
            "Paso: 241, Costo: 0.32979220151901245\n",
            "Paso: 242, Costo: 0.32961490750312805\n",
            "Paso: 243, Costo: 0.3294387757778168\n",
            "Paso: 244, Costo: 0.3292636573314667\n",
            "Paso: 245, Costo: 0.3290896415710449\n",
            "Paso: 246, Costo: 0.3289166986942291\n",
            "Paso: 247, Costo: 0.3287447988986969\n",
            "Paso: 248, Costo: 0.3285738527774811\n",
            "Paso: 249, Costo: 0.3284039795398712\n",
            "Paso: 250, Costo: 0.32823508977890015\n",
            "Paso: 251, Costo: 0.32806721329689026\n",
            "Paso: 252, Costo: 0.32790032029151917\n",
            "Paso: 253, Costo: 0.32773444056510925\n",
            "Paso: 254, Costo: 0.32756948471069336\n",
            "Paso: 255, Costo: 0.32740554213523865\n",
            "Paso: 256, Costo: 0.32724252343177795\n",
            "Paso: 257, Costo: 0.3270803987979889\n",
            "Paso: 258, Costo: 0.3269192576408386\n",
            "Paso: 259, Costo: 0.32675901055336\n",
            "Paso: 260, Costo: 0.32659974694252014\n",
            "Paso: 261, Costo: 0.32644131779670715\n",
            "Paso: 262, Costo: 0.3262837827205658\n",
            "Paso: 263, Costo: 0.32612714171409607\n",
            "Paso: 264, Costo: 0.32597142457962036\n",
            "Paso: 265, Costo: 0.3258165419101715\n",
            "Paso: 266, Costo: 0.3256625235080719\n",
            "Paso: 267, Costo: 0.3255093991756439\n",
            "Paso: 268, Costo: 0.325357049703598\n",
            "Paso: 269, Costo: 0.32520559430122375\n",
            "Paso: 270, Costo: 0.32505494356155396\n",
            "Paso: 271, Costo: 0.3249051570892334\n",
            "Paso: 272, Costo: 0.32475611567497253\n",
            "Paso: 273, Costo: 0.3246079385280609\n",
            "Paso: 274, Costo: 0.32446053624153137\n",
            "Paso: 275, Costo: 0.3243139684200287\n",
            "Paso: 276, Costo: 0.3241681754589081\n",
            "Paso: 277, Costo: 0.3240230977535248\n",
            "Paso: 278, Costo: 0.32387885451316833\n",
            "Paso: 279, Costo: 0.32373538613319397\n",
            "Paso: 280, Costo: 0.3235926330089569\n",
            "Paso: 281, Costo: 0.32345065474510193\n",
            "Paso: 282, Costo: 0.32330942153930664\n",
            "Paso: 283, Costo: 0.32316893339157104\n",
            "Paso: 284, Costo: 0.32302916049957275\n",
            "Paso: 285, Costo: 0.32289013266563416\n",
            "Paso: 286, Costo: 0.32275182008743286\n",
            "Paso: 287, Costo: 0.32261422276496887\n",
            "Paso: 288, Costo: 0.3224772810935974\n",
            "Paso: 289, Costo: 0.32234108448028564\n",
            "Paso: 290, Costo: 0.3222056031227112\n",
            "Paso: 291, Costo: 0.32207080721855164\n",
            "Paso: 292, Costo: 0.3219366669654846\n",
            "Paso: 293, Costo: 0.32180318236351013\n",
            "Paso: 294, Costo: 0.32167041301727295\n",
            "Paso: 295, Costo: 0.3215382695198059\n",
            "Paso: 296, Costo: 0.3214068114757538\n",
            "Paso: 297, Costo: 0.3212760388851166\n",
            "Paso: 298, Costo: 0.3211458623409271\n",
            "Paso: 299, Costo: 0.3210163414478302\n",
            "Paso: 300, Costo: 0.3208874762058258\n",
            "Paso: 301, Costo: 0.32075920701026917\n",
            "Paso: 302, Costo: 0.32063159346580505\n",
            "Paso: 303, Costo: 0.32050463557243347\n",
            "Paso: 304, Costo: 0.32037824392318726\n",
            "Paso: 305, Costo: 0.3202524781227112\n",
            "Paso: 306, Costo: 0.32012730836868286\n",
            "Paso: 307, Costo: 0.3200027346611023\n",
            "Paso: 308, Costo: 0.31987878680229187\n",
            "Paso: 309, Costo: 0.3197554647922516\n",
            "Paso: 310, Costo: 0.3196326494216919\n",
            "Paso: 311, Costo: 0.31951043009757996\n",
            "Paso: 312, Costo: 0.31938880681991577\n",
            "Paso: 313, Costo: 0.31926777958869934\n",
            "Paso: 314, Costo: 0.3191473186016083\n",
            "Paso: 315, Costo: 0.3190273642539978\n",
            "Paso: 316, Costo: 0.3189080059528351\n",
            "Paso: 317, Costo: 0.31878921389579773\n",
            "Paso: 318, Costo: 0.31867095828056335\n",
            "Paso: 319, Costo: 0.31855326890945435\n",
            "Paso: 320, Costo: 0.3184361457824707\n",
            "Paso: 321, Costo: 0.3183194696903229\n",
            "Paso: 322, Costo: 0.3182034194469452\n",
            "Paso: 323, Costo: 0.3180878460407257\n",
            "Paso: 324, Costo: 0.3179728388786316\n",
            "Paso: 325, Costo: 0.3178583085536957\n",
            "Paso: 326, Costo: 0.31774431467056274\n",
            "Paso: 327, Costo: 0.3176308274269104\n",
            "Paso: 328, Costo: 0.31751787662506104\n",
            "Paso: 329, Costo: 0.3174054026603699\n",
            "Paso: 330, Costo: 0.3172934353351593\n",
            "Paso: 331, Costo: 0.3171819746494293\n",
            "Paso: 332, Costo: 0.31707096099853516\n",
            "Paso: 333, Costo: 0.31696054339408875\n",
            "Paso: 334, Costo: 0.3168505132198334\n",
            "Paso: 335, Costo: 0.316741019487381\n",
            "Paso: 336, Costo: 0.3166319727897644\n",
            "Paso: 337, Costo: 0.3165234327316284\n",
            "Paso: 338, Costo: 0.31641536951065063\n",
            "Paso: 339, Costo: 0.31630775332450867\n",
            "Paso: 340, Costo: 0.3162005841732025\n",
            "Paso: 341, Costo: 0.31609389185905457\n",
            "Paso: 342, Costo: 0.3159877061843872\n",
            "Paso: 343, Costo: 0.3158819079399109\n",
            "Paso: 344, Costo: 0.3157765865325928\n",
            "Paso: 345, Costo: 0.31567174196243286\n",
            "Paso: 346, Costo: 0.3155673146247864\n",
            "Paso: 347, Costo: 0.3154633343219757\n",
            "Paso: 348, Costo: 0.31535977125167847\n",
            "Paso: 349, Costo: 0.31525668501853943\n",
            "Paso: 350, Costo: 0.3151540160179138\n",
            "Paso: 351, Costo: 0.31505176424980164\n",
            "Paso: 352, Costo: 0.3149499297142029\n",
            "Paso: 353, Costo: 0.31484857201576233\n",
            "Paso: 354, Costo: 0.3147476017475128\n",
            "Paso: 355, Costo: 0.31464701890945435\n",
            "Paso: 356, Costo: 0.3145468831062317\n",
            "Paso: 357, Costo: 0.3144471347332001\n",
            "El entrenamiento converge\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CAhCnuPToty",
        "colab_type": "text"
      },
      "source": [
        "Se determina la precision del modelo con los datos del conjunto de prueba"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7fpKmxLY866",
        "colab_type": "code",
        "outputId": "49265262-5848-4571-f78c-2b543b3e5ac6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32))*100\n",
        "resul=tf.print(\"La precision del modelo es:\", accuracy)\n",
        "resul=sess.run(resul, feed_dict={x: numpy_images_test, y_: labels_test} )"
      ],
      "execution_count": 368,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "La precision del modelo es: 91.61\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iV56p0HJW6dg",
        "colab_type": "text"
      },
      "source": [
        "###Inferencia\n",
        "Se selecciona una imagen aleatoria del conjunto de prueba y se hace pasar a traves del modelo (ecuacion que define al Perceptron con los pesos y bias obtenidos en el entrenamiento)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgbMOOLbTwnY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "26827e35-847a-441d-ebe6-65af316c0dda"
      },
      "source": [
        "image_index = 5672\n",
        "img=numpy_images_test[image_index].reshape(28,28)\n",
        "plt.imshow(img, cmap=\"gray\")\n",
        "\n",
        "prediction=tf.nn.softmax( tf.add( tf.matmul( tf.cast(tf.expand_dims( numpy_images_test[image_index],0), tf.float32), tf.cast(new_W, tf.float32) ), new_b ) )\n",
        "encode=tf.argmax(prediction, axis=1)#la predicion obtenida es un vector binario, se debe codificar a un valor numérico\n",
        "res=tf.print(\"El valor inferido es:\", encode)\n",
        "sess.run(res)"
      ],
      "execution_count": 387,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "El valor inferido es: [6]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHwiK8ANBEX4gKMIPBEX4gaAIPxDU/wD/4FMqFaFerwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OAmO9qSEn2S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess.close() "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}